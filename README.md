RAG works by downloading Ollama, a tool that allows you to run and prompt an LLM locally as opposed to accessing an LLM online via a commercially available tool like Chat-GPT. After downloading Ollama, you will use Python to run an LLM on your computer and Streamlit to establish a user interface (UI) in a web browser that will allow you to prompt the LLM.<br>

The notebook will guide you through the process of downloading/running all of the necessary tools/packages and establishing the UI from which to prompt the LLM. 
